---
title: "OpenAI API: Configura√ß√£o de Integra√ß√£o | Antigravity-Manager"
sidebarTitle: "Conecte SDK OpenAI em 5 Minutos"
subtitle: "OpenAI API: Configura√ß√£o de Integra√ß√£o"
description: "Aprenda configura√ß√£o de integra√ß√£o da API compat√≠vel OpenAI. Domine convers√£o de rota, configura√ß√£o base_url e solu√ß√£o 401/404/429, use rapidamente Antigravity Tools."
tags:
  - "OpenAI"
  - "API Proxy"
  - "Chat Completions"
  - "Responses"
  - "Antigravity Tools"
prerequisite:
  - "start-proxy-and-first-client"
  - "start-add-account"
order: 1
---

# API Compat√≠vel OpenAI: Estrat√©gia de Implementa√ß√£o de /v1/chat/completions e /v1/responses

Voc√™ usar√° esta **API Compat√≠vel OpenAI** para conectar diretamente SDK/Clientes OpenAI existentes ao gateway local do Antigravity Tools, focando em rodar `/v1/chat/completions` e `/v1/responses`, e aprender a usar cabe√ßalhos de resposta para solu√ß√£o r√°pida de problemas.

## O que voc√™ poder√° fazer ap√≥s completar

- Conectar-se diretamente ao gateway local do Antigravity Tools usando OpenAI SDK (ou curl)
- Rodar `/v1/chat/completions` (incluindo `stream: true`) e `/v1/responses`
- Entender lista de modelos de `/v1/models`, e `X-Mapped-Model` nos cabe√ßalhos de resposta
- Ao encontrar 401/404/429, saber onde investigar primeiro

## Seu dilema atual

Muitos clientes/SDKs s√≥ reconhecem formato de interface OpenAI: URL fixa, campos JSON fixos, formato SSE de fluxo fixo.
O objetivo do Antigravity Tools n√£o √©ËÆ©‰Ω† modificar o cliente, mas deixar o cliente "pensar que est√° chamando OpenAI", na verdade converte solicita√ß√£o para chamada upstream interna, depois converte resultado de volta para formato OpenAI.

## Quando usar esta t√©cnica

- Voc√™ j√° tem um monte de ferramentas que s√≥ suportam OpenAI (plugins IDE, scripts, bots, SDKs), n√£o quer escrever nova integra√ß√£o para cada um
- Voc√™ quer usar uniformemente `base_url` para bater solicita√ß√µes no gateway local (ou LAN), depois deixar gateway fazer agendamento de conta, retry e monitoramento

## üéí Prepara√ß√£o antes de come√ßar

::: warning Pr√©-requisitos
- Voc√™ j√° iniciou servi√ßo de proxy reverso na p√°gina "API Proxy" do Antigravity Tools, e anotou porta (ex: `8045`)
- Voc√™ j√° adicionou pelo menos uma conta dispon√≠vel, caso contr√°rio proxy n√£o consegue obter token upstream
:::

::: info Como trazer autentica√ß√£o?
Quando voc√™ habilita `proxy.auth_mode` e configura `proxy.api_key`, solicita√ß√µes precisam trazer API Key.

Middleware do Antigravity Tools ler√° prioridade `Authorization`, tamb√©m compat√≠vel com `x-api-key`, `x-goog-api-key`. (Implementa√ß√£o em `src-tauri/src/proxy/middleware/auth.rs`)
:::

## O que √© API Compat√≠vel OpenAI?

**API Compat√≠vel OpenAI** √© um conjunto de rotas HTTP e protocolos JSON/SSE que "parecem OpenAI". Cliente envia em formato de solicita√ß√£o OpenAI para gateway local, gateway converte solicita√ß√£o para chamada upstream interna, depois converte resposta upstream de volta para estrutura de resposta OpenAI, permitindo que SDKs OpenAI existentes b√°sicos funcionem sem modifica√ß√£o.

### Vis√£o r√°pida de endpoints compat√≠veis (relacionados a esta li√ß√£o)

| Endpoint | Finalidade | Evid√™ncia de c√≥digo |
|--- | --- | ---|
| `POST /v1/chat/completions` | Chat Completions (incluindo fluxo) | Registro de rota em `src-tauri/src/proxy/server.rs`; `src-tauri/src/proxy/handlers/openai.rs` |
| `POST /v1/completions` | Legacy Completions (reusa mesmo processador) | Registro de rota em `src-tauri/src/proxy/server.rs` |
| `POST /v1/responses` | Compatibilidade Responses/Codex CLI (reusa mesmo processador) | Registro de rota em `src-tauri/src/proxy/server.rs` (coment√°rio: compatibilidade Codex CLI) |
| `GET /v1/models` | Retorna lista de modelos (incluindo mapeamento personalizado + gera√ß√£o din√¢mica) | `src-tauri/src/proxy/handlers/openai.rs` + `src-tauri/src/proxy/common/model_mapping.rs` |

## Siga-me

### Passo 1: Confirme servi√ßo vivo com curl (/healthz + /v1/models)

**Por que**
Primeiro elimine problemas b√°sicos como "servi√ßo n√£o iniciado/porta errada/bloqueada por firewall".

```bash
 # 1) Verifica√ß√£o de sa√∫de
 curl -s http://127.0.0.1:8045/healthz

 # 2) Puxar lista de modelos
 curl -s http://127.0.0.1:8045/v1/models
```

**Voc√™ deve ver**: `/healthz` retorna algo como `{"status":"ok"}`; `/v1/models` retorna `{"object":"list","data":[...]}`.

### Passo 2: Chame /v1/chat/completions com OpenAI Python SDK

**Por que**
Este passo prova que cadeia completa "OpenAI SDK ‚Üí gateway local ‚Üí upstream ‚Üí convers√£o de resposta OpenAI" est√° conectada.

```python
import openai

client = openai.OpenAI(
    api_key="sk-antigravity",
    base_url="http://127.0.0.1:8045/v1",
)

response = client.chat.completions.create(
    model="gemini-3-flash",
    messages=[{"role": "user", "content": "Ol√°, por favor se apresente"}],
)

print(response.choices[0].message.content)
```

**Voc√™ deve ver**: Terminal imprime um trecho de texto de resposta de modelo.

### Passo 3: Abra stream, confirme retorno de fluxo SSE

**Por que**
Muitos clientes dependem de protocolo SSE do OpenAI (`Content-Type: text/event-stream`). Este passo confirma que cadeia de fluxo e formato de evento est√£o dispon√≠veis.

```bash
curl -N http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "stream": true,
    "messages": [
      {"role": "user", "content": "Explique em 3 frases o que √© gateway de proxy reverso local"}
    ]
  }'
```

**Voc√™ deve ver**: Terminal continuamente sai linhas come√ßando com `data: { ... }`, terminando com `data: [DONE]`.

### Passo 4: Use /v1/responses (estilo Codex/Responses) rodar uma solicita√ß√£o

**Por que**
Algumas ferramentas v√£o em `/v1/responses` ou usar√£o campos como `instructions`, `input` no corpo da solicita√ß√£o. Este projeto "normalizar√°" esse tipo de solicita√ß√£o para `messages` depois reusar mesma l√≥gica de convers√£o. (Processador em `src-tauri/src/proxy/handlers/openai.rs`)

```bash
curl -s http://127.0.0.1:8045/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "instructions": "Voc√™ √© um revisor de c√≥digo rigoroso.",
    "input": "Aponte o bug mais prov√°vel deste c√≥digo:\n\nfunction add(a, b) { return a - b }"
  }'
```

**Voc√™ deve ver**: Corpo de resposta √© um objeto no estilo OpenAI (este projeto converte resposta Gemini para `choices[].message.content` do OpenAI).

### Passo 5: Confirme roteamento de modelo ativo (veja cabe√ßalho de resposta X-Mapped-Model)

**Por que**
O `model` que voc√™ escreve no cliente n√£o √© necessariamente o "modelo f√≠sico" realmente chamado. Gateway far√° primeiro mapeamento de modelo (incluindo mapeamento personalizado/curinga, veja [Roteamento de Modelos: Mapeamento Personalizado, Prioridade de Curinga e Estrat√©gias Padr√£o](/pt/lbjlaq/Antigravity-Manager/advanced/model-router/)), e colocar√° resultado final no cabe√ßalho de resposta, facilitando sua solu√ß√£o de problemas.

```bash
curl -i http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "ol√°"}]
  }'
```

**Voc√™ deve ver**: Cabe√ßalho de resposta cont√©m `X-Mapped-Model: ...` (ex: mapeado para `gemini-2.5-flash`), e pode tamb√©m conter `X-Account-Email: ...`.

## Ponto de verifica√ß√£o ‚úÖ

- `GET /healthz` retorna `{"status":"ok"}` (ou JSON equivalente)
- `GET /v1/models` retorna `object=list` e `data` √© array
- Solicita√ß√£o `/v1/chat/completions` sem fluxo consegue obter `choices[0].message.content`
- Quando `stream: true`, consegue receber SSE, terminando com `[DONE]`
- `curl -i` consegue ver cabe√ßalho de resposta `X-Mapped-Model`

## Aviso sobre armadilhas

### 1) Base URL errado causando 404 (mais comum)

- Em exemplos de OpenAI SDK, `base_url` precisa terminar com `/v1` (veja exemplo Python no README do projeto).
- Alguns clientes "empilham caminhos". Por exemplo, README menciona explicitamente: Kilo Code no modo OpenAI pode concatenar caminho n√£o padr√£o como `/v1/chat/completions/responses`, disparando 404.

### 2) 401: n√£o √© upstream caiu, √© voc√™ n√£o trouxe key ou modo errado

Quando "modo v√°lido" de estrat√©gia de autentica√ß√£o n√£o √© `off`, middleware validar√° cabe√ßalhos de solicita√ß√£o: `Authorization: Bearer <proxy.api_key>`, tamb√©m compat√≠vel com `x-api-key`, `x-goog-api-key`. (Implementa√ß√£o em `src-tauri/src/proxy/middleware/auth.rs`)

::: tip Dica de modo de autentica√ß√£o
Quando `auth_mode = auto`, decidir√° automaticamente baseado em `allow_lan_access`:
- `allow_lan_access = true` ‚Üí modo v√°lido √© `all_except_health` (todos exceto `/healthz` precisam autentica√ß√£o)
- `allow_lan_access = false` ‚Üí modo v√°lido √© `off` (acesso local n√£o precisa autentica√ß√£o)
:::

### 3) 429/503/529: proxy far√° retry + rota√ß√£o de conta, mas tamb√©m pode "esgotar pool"

Processador OpenAI embutido no m√°ximo 3 tentativas (e limitado por tamanho de pool de contas), encontrando certos erros esperar√°/rotacionar√° conta para retry. (Implementa√ß√£o em `src-tauri/src/proxy/handlers/openai.rs`)

## Resumo da li√ß√£o

- `/v1/chat/completions` √© ponto de entrada mais geral, `stream: true` vai via SSE
- `/v1/responses` e `/v1/completions` usam mesmo processador de compatibilidade, n√∫cleo √© primeiro normalizar solicita√ß√£o para `messages`
- `X-Mapped-Model` ajuda confirmar resultado de mapeamento "nome de modelo cliente ‚Üí modelo f√≠sico final"

## Pr√≥ximo aviso de li√ß√£o

> Na pr√≥xima li√ß√£o continuaremos ver **API Compat√≠vel Anthropic: /v1/messages e Contratos Chave do Claude Code** (cap√≠tulo correspondente: `platforms-anthropic`).

---

## Ap√™ndice: Refer√™ncia de c√≥digo-fonte

<details>
<summary><strong>Clique para expandir localiza√ß√£o do c√≥digo-fonte</strong></summary>

> Atualizado em: 2026-01-23

| Funcionalidade | Caminho do arquivo | Linha |
|--- | --- | ---|
| Registro de rota OpenAI (incluindo /v1/responses) | [`src-tauri/src/proxy/server.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/server.rs#L120-L194) | 120-194 |
| Processador Chat Completions (incluindo detec√ß√£o de formato Responses) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L70-L462) | 70-462 |
| Processador /v1/completions e /v1/responses (normaliza√ß√£o Codex/Responses + retry/rota√ß√£o) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L464-L1080) | 464-1080 |
| Retorno de /v1/models (lista de modelos din√¢mica) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L1082-L1102) | 1082-1102 |
| Estrutura de dados de solicita√ß√£o OpenAI (messages/instructions/input/size/quality) | [`src-tauri/src/proxy/mappers/openai/models.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/models.rs#L7-L38) | 7-38 |
|--- | --- | ---|
|--- | --- | ---|
| Mapeamento de modelo e prioridade de curinga (exato > curinga > padr√£o) | [`src-tauri/src/proxy/common/model_mapping.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/common/model_mapping.rs#L180-L228) | 180-228 |
|--- | --- | ---|

**Constantes principais**:
- `MAX_RETRY_ATTEMPTS = 3`: n√∫mero m√°ximo de tentativas de protocolo OpenAI (incluindo rota√ß√£o) (veja `src-tauri/src/proxy/handlers/openai.rs`)

**Fun√ß√µes principais**:
- `transform_openai_request(...)`: converte corpo de solicita√ß√£o OpenAI em solicita√ß√£o upstream interna (veja `src-tauri/src/proxy/mappers/openai/request.rs`)
- `transform_openai_response(...)`: converte resposta upstream em `choices`/`usage` do OpenAI (veja `src-tauri/src/proxy/mappers/openai/response.rs`)

</details>
