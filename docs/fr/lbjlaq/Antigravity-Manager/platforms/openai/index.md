---
title: "API OpenAI : Configuration de l'int√©gration | Antigravity-Manager"
sidebarTitle: "Connecter le SDK OpenAI en 5 minutes"
subtitle: "API OpenAI : Configuration de l'int√©gration"
description: "Apprenez √† configurer l'int√©gration de l'API compatible OpenAI. Ma√Ætrisez la conversion des routes, la configuration du base_url et le d√©pannage des erreurs 401/404/429, pour utiliser rapidement les outils Antigravity."
tags:
  - "OpenAI"
  - "Proxy API"
  - "Chat Completions"
  - "Responses"
  - "Outils Antigravity"
prerequisite:
  - "start-proxy-and-first-client"
  - "start-add-account"
order: 1
---

# API compatible OpenAI : Strat√©gies de d√©ploiement pour /v1/chat/completions et /v1/responses

Vous utiliserez cette **API compatible OpenAI** pour connecter directement les SDK/clients OpenAI existants √† la passerelle locale des outils Antigravity. L'objectif principal est de faire fonctionner `/v1/chat/completions` et `/v1/responses`, et d'apprendre √† r√©soudre les probl√®mes rapidement gr√¢ce aux en-t√™tes de r√©ponse.

## Ce que vous pourrez faire apr√®s ce cours

- Connecter le SDK OpenAI (ou curl) directement √† la passerelle locale des outils Antigravity
- Faire fonctionner `/v1/chat/completions` (avec `stream: true`) et `/v1/responses`
- Comprendre la liste des mod√®les de `/v1/models` et l'en-t√™te de r√©ponse `X-Mapped-Model`
- Savoir quoi v√©rifier en priorit√© en cas d'erreurs 401/404/429

## Votre situation actuelle

De nombreux clients/SDK ne reconnaissent que la forme des interfaces OpenAI : des URL fixes, des champs JSON fixes et un format de flux SSE fixe.
L'objectif des outils Antigravity n'est pas de vous faire modifier vos clients, mais de faire croire aux clients qu'ils communiquent avec OpenAI, tout en convertissant les requ√™tes en appels en amont internes, puis en reconvertissant les r√©sultats au format OpenAI.

## Quand utiliser cette approche

- Vous disposez d√©j√† d'un ensemble d'outils ne supportant que OpenAI (plugins IDE, scripts, bots, SDK) et vous ne souhaitez pas √©crire une nouvelle int√©gration pour chacun
- Vous souhaitez utiliser un `base_url` unique pour diriger les requ√™tes vers une passerelle locale (ou r√©seau local), la passerelle g√©rant ensuite la planification des comptes, les nouvelles tentatives et la surveillance

## üéí Pr√©paratifs avant de commencer

::: warning Conditions pr√©alables
- Vous avez d√©j√† d√©marr√© le service de proxy inverse dans la page "API Proxy" d'Antigravity Tools et avez not√© le port (par exemple `8045`)
- Vous avez ajout√© au moins un compte disponible, sinon le proxy ne peut pas obtenir de jeton en amont
:::

::: info Comment transmettre l'authentification ?
Lorsque vous activez `proxy.auth_mode` et configurez `proxy.api_key`, la requ√™te doit inclure une API Key.

Le middleware des outils Antigravity lit d'abord `Authorization`, et est √©galement compatible avec `x-api-key` et `x-goog-api-key`. (Impl√©mentation dans `src-tauri/src/proxy/middleware/auth.rs`)
:::

## Qu'est-ce que l'API compatible OpenAI ?

L'**API compatible OpenAI** est un ensemble de routes HTTP et de protocoles JSON/SSE qui "ressemblent √† OpenAI". Les clients envoient des requ√™tes au format OpenAI vers la passerelle locale, qui convertit ensuite les requ√™tes en appels en amont internes et reconvertit les r√©ponses en amont au format de r√©ponse OpenAI, permettant aux SDK OpenAI existants de fonctionner sans modification majeure.

### Aper√ßu des points de terminaison compatibles (li√©s √† ce cours)

| Point de terminaison | Utilisation | Preuve dans le code |
| --- | --- | --- |
| `POST /v1/chat/completions` | Chat Completions (avec flux) | Enregistrement de route dans `src-tauri/src/proxy/server.rs` ; `src-tauri/src/proxy/handlers/openai.rs` |
| `POST /v1/completions` | Completions h√©rit√©es (r√©utilisation du m√™me gestionnaire) | Enregistrement de route dans `src-tauri/src/proxy/server.rs` |
| `POST /v1/responses` | Compatibilit√© Responses/Codex CLI (r√©utilisation du m√™me gestionnaire) | Enregistrement de route dans `src-tauri/src/proxy/server.rs` (commentaire : compatibilit√© Codex CLI) |
| `GET /v1/models` | Retourne la liste des mod√®les (avec mappage personnalis√© + g√©n√©ration dynamique) | `src-tauri/src/proxy/handlers/openai.rs` + `src-tauri/src/proxy/common/model_mapping.rs` |

## Suivez les √©tapes

### √âtape 1 : V√©rifiez que le service fonctionne avec curl (/healthz + /v1/models)

**Pourquoi**
√âliminez d'abord les probl√®mes √©l√©mentaires comme "service non d√©marr√©/port incorrect/bloqu√© par le pare-feu".

```bash
 # 1) V√©rification de sant√©
curl -s http://127.0.0.1:8045/healthz

 # 2) R√©cup√©ration de la liste des mod√®les
curl -s http://127.0.0.1:8045/v1/models
```

**Ce que vous devriez voir** : `/healthz` retourne quelque chose comme `{"status":"ok"}` ; `/v1/models` retourne `{"object":"list","data":[...]}`.

### √âtape 2 : Appelez /v1/chat/completions avec le SDK OpenAI Python

**Pourquoi**
Cette √©tape prouve que toute la cha√Æne "SDK OpenAI ‚Üí passerelle locale ‚Üí en amont ‚Üí conversion de r√©ponse OpenAI" fonctionne.

```python
import openai

client = openai.OpenAI(
    api_key="sk-antigravity",
    base_url="http://127.0.0.1:8045/v1",
)

response = client.chat.completions.create(
    model="gemini-3-flash",
    messages=[{"role": "user", "content": "Bonjour, pr√©sentez-vous"}],
)

print(response.choices[0].message.content)
```

**Ce que vous devriez voir** : Le terminal affiche un texte de r√©ponse du mod√®le.

### √âtape 3 : Activez le flux pour confirmer le retour SSE

**Pourquoi**
De nombreux clients d√©pendent du protocole SSE d'OpenAI (`Content-Type: text/event-stream`). Cette √©tape confirme que le flux et le format des √©v√©nements sont disponibles.

```bash
curl -N http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "stream": true,
    "messages": [
      {"role": "user", "content": "Expliquez en trois phrases ce qu'est un proxy inverse local"}
    ]
  }'
```

**Ce que vous devriez voir** : Le terminal affiche en continu des lignes commen√ßant par `data: { ... }`, se terminant par `data: [DONE]`.

### √âtape 4 : Utilisez /v1/responses (style Codex/Responses) pour une requ√™te

**Pourquoi**
Certains outils utilisent `/v1/responses` ou des champs comme `instructions` et `input` dans le corps de la requ√™te. Ce projet "normalise" ce type de requ√™te en `messages` puis r√©utilise la m√™me logique de conversion. (Gestionnaire dans `src-tauri/src/proxy/handlers/openai.rs`)

```bash
curl -s http://127.0.0.1:8045/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "instructions": "Vous √™tes un r√©viseur de code rigoureux.",
    "input": "Indiquez le bug le plus probable dans le code suivant :\n\nfunction add(a, b) { return a - b }"
  }'
```

**Ce que vous devriez voir** : Le corps de la r√©ponse est un objet au format OpenAI (ce projet convertira la r√©ponse Gemini en `choices[].message.content` d'OpenAI).

### √âtape 5 : V√©rifiez que le routage des mod√®les fonctionne (voir l'en-t√™te X-Mapped-Model)

**Pourquoi**
Le `model` que vous √©crivez dans le client n'est pas n√©cessairement le "mod√®le physique" r√©ellement appel√©. La passerelle effectue d'abord un mappage de mod√®les (incluant le mappage personnalis√©/les caract√®res g√©n√©riques, voir [Routage des mod√®les : mappage personnalis√©, priorit√© des caract√®res g√©n√©riques et strat√©gies pr√©d√©finies](/fr/lbjlaq/Antigravity-Manager/advanced/model-router/)), puis place le r√©sultat final dans l'en-t√™te de r√©ponse pour faciliter le d√©pannage.

```bash
curl -i http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "salut"}]
  }'
```

**Ce que vous devriez voir** : L'en-t√™te de r√©ponse contient `X-Mapped-Model: ...` (par exemple mapp√© vers `gemini-2.5-flash`), et peut √©galement contenir `X-Account-Email: ...`.

## Point de contr√¥le ‚úÖ

- `GET /healthz` retourne `{"status":"ok"}` (ou JSON √©quivalent)
- `GET /v1/models` retourne `object=list` et `data` est un tableau
- La requ√™te non flux `/v1/chat/completions` peut obtenir `choices[0].message.content`
- Avec `stream: true`, vous recevez SSE, se terminant par `[DONE]`
- `curl -i` peut voir l'en-t√™te de r√©ponse `X-Mapped-Model`

## Pi√®ges √† √©viter

### 1) Base URL incorrect entra√Æne 404 (le plus courant)

- Dans les exemples du SDK OpenAI, `base_url` doit se terminer par `/v1` (voir l'exemple Python du README du projet).
- Certains clients "empilent les chemins". Par exemple, le README mentionne explicitement : Kilo Code en mode OpenAI peut construire des chemins non standards comme `/v1/chat/completions/responses`, d√©clenchant ainsi une erreur 404.

### 2) 401 : ce n'est pas que l'en amont est en panne, c'est que vous n'avez pas fourni de cl√© ou que le mode est incorrect

Lorsque le "mode valide" de la strat√©gie d'authentification n'est pas `off`, le middleware v√©rifie les en-t√™tes de requ√™te : `Authorization: Bearer <proxy.api_key>`, et est √©galement compatible avec `x-api-key` et `x-goog-api-key`. (Impl√©mentation dans `src-tauri/src/proxy/middleware/auth.rs`)

::: tip Indication sur le mode d'authentification
Avec `auth_mode = auto`, la d√©cision est automatique selon `allow_lan_access` :
- `allow_lan_access = true` ‚Üí mode valide = `all_except_health` (authentification requise sauf pour `/healthz`)
- `allow_lan_access = false` ‚Üí mode valide = `off` (pas d'authentification pour l'acc√®s local)
:::

### 3) 429/503/529 : le proxy fait des nouvelles tentatives + rotation de comptes, mais le "pool peut √™tre √©puis√©"

Le gestionnaire OpenAI int√®gre jusqu'√† 3 tentatives (et est limit√© par la taille du pool de comptes), en cas de certaines erreurs il attend/rotate les comptes pour r√©essayer. (Impl√©mentation dans `src-tauri/src/proxy/handlers/openai.rs`)

## R√©sum√© du cours

- `/v1/chat/completions` est le point d'int√©gration le plus universel, `stream: true` utilise SSE
- `/v1/responses` et `/v1/completions` utilisent le m√™me gestionnaire de compatibilit√©, la cl√© est de normaliser d'abord la requ√™te en `messages`
- `X-Mapped-Model` vous aide √† confirmer le r√©sultat du mappage "nom du mod√®le client ‚Üí mod√®le physique final"

## Prochain cours

> Dans le prochain cours, nous continuerons avec **API compatible Anthropic : /v1/messages et les contrats cl√©s de Claude Code** (chapitre correspondant : `platforms-anthropic`).

---

## Annexe : R√©f√©rences du code source

<details>
<summary><strong>Cliquez pour voir les emplacements du code source</strong></summary>

> Derni√®re mise √† jour : 2026-01-23

| Fonctionnalit√© | Chemin du fichier | Lignes |
| --- | --- | --- |
| Enregistrement des routes OpenAI (incluant /v1/responses) | [`src-tauri/src/proxy/server.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/server.rs#L120-L194) | 120-194 |
| Gestionnaire Chat Completions (incluant la d√©tection du format Responses) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L70-L462) | 70-462 |
| Gestionnaire /v1/completions et /v1/responses (normalisation Codex/Responses + nouvelles tentatives/rotation) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L464-L1080) | 464-1080 |
| Retour de /v1/models (liste dynamique de mod√®les) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L1082-L1102) | 1082-1102 |
| Structure de donn√©es de requ√™te OpenAI (messages/instructions/input/size/quality) | [`src-tauri/src/proxy/mappers/openai/models.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/models.rs#L7-L38) | 7-38 |
| Conversion OpenAI -> Gemini de la requ√™te (systemInstruction/thinkingConfig/tools) | [`src-tauri/src/proxy/mappers/openai/request.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/request.rs#L6-L553) | 6-553 |
| Conversion Gemini -> OpenAI de la r√©ponse (choices/usageMetadata) | [`src-tauri/src/proxy/mappers/openai/response.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/response.rs#L5-L214) | 5-214 |
| Mappage de mod√®les et priorit√© des caract√®res g√©n√©riques (exact > g√©n√©rique > d√©faut) | [`src-tauri/src/proxy/common/model_mapping.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/common/model_mapping.rs#L180-L228) | 180-228 |
| Middleware d'authentification (Authorization/x-api-key/x-goog-api-key) | [`src-tauri/src/proxy/middleware/auth.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/middleware/auth.rs#L15-L77) | 15-77 |

**Constantes cl√©s** :
- `MAX_RETRY_ATTEMPTS = 3` : nombre maximum de tentatives pour le protocole OpenAI (incluant la rotation) (voir `src-tauri/src/proxy/handlers/openai.rs`)

**Fonctions cl√©s** :
- `transform_openai_request(...)` : convertit le corps de la requ√™te OpenAI en requ√™te en amont interne (voir `src-tauri/src/proxy/mappers/openai/request.rs`)
- `transform_openai_response(...)` : convertit la r√©ponse en amont en `choices`/`usage` d'OpenAI (voir `src-tauri/src/proxy/mappers/openai/response.rs`)

</details>
