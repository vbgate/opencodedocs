---
title: "API de OpenAI: configuraci√≥n de integraci√≥n | Antigravity-Manager"
sidebarTitle: "Conecta el SDK de OpenAI en 5 minutos"
subtitle: "API de OpenAI: configuraci√≥n de integraci√≥n"
description: "Aprende la configuraci√≥n de integraci√≥n de API compatible con OpenAI. Domina la conversi√≥n de rutas, la configuraci√≥n de base_url y la soluci√≥n de problemas de 401/404/429, para usar r√°pidamente Antigravity Tools."
tags:
  - "OpenAI"
  - "Proxy de API"
  - "Chat Completions"
  - "Responses"
  - "Antigravity Tools"
prerequisite:
  - "start-proxy-and-first-client"
  - "start-add-account"
order: 1
---

# API compatible con OpenAI: estrategia de implementaci√≥n para /v1/chat/completions y /v1/responses

Usar√°s esta **API compatible con OpenAI** para conectar directamente el SDK o cliente de OpenAI existente al gateway local de Antigravity Tools. El objetivo principal es ejecutar `/v1/chat/completions` y `/v1/responses`, y aprender a solucionar problemas r√°pidamente usando los encabezados de respuesta.

## Qu√© podr√°s hacer al completar este tutorial

- Conectar directamente el SDK de OpenAI (o curl) al gateway local de Antigravity Tools
- Ejecutar `/v1/chat/completions` (incluyendo `stream: true`) y `/v1/responses`
- Entender la lista de modelos de `/v1/models` y el `X-Mapped-Model` en los encabezados de respuesta
- Saber qu√© verificar primero cuando encuentres errores 401/404/429

## Tu problema actual

Muchos clientes/SDK solo reconocen la forma de la interfaz de OpenAI: URL fija, campos JSON fijos y formato de flujo SSE fijo. El objetivo de Antigravity Tools no es que modifiques el cliente, sino que el cliente "cree que est√° llamando a OpenAI", mientras la solicitud se convierte en una llamada a upstream interna y el resultado se transforma de nuevo al formato de OpenAI.

## Cu√°ndo usar este enfoque

- Ya tienes un conjunto de herramientas que solo soportan OpenAI (plugins de IDE, scripts, bots, SDK) y no quieres escribir una nueva integraci√≥n para cada uno
- Deseas usar `base_url` para enviar todas las solicitudes al gateway local (o LAN), y dejar que el gateway gestione la programaci√≥n de cuentas, reintentos y monitoreo

## üéí Preparativos antes de comenzar

::: warning Requisitos previos
- Ya has iniciado el servicio de proxy inverso en la p√°gina "API Proxy" de Antigravity Tools y anotado el puerto (por ejemplo, `8045`)
- Ya has agregado al menos una cuenta disponible, de lo contrario el proxy inverso no obtendr√° el token de upstream
:::

::: info ¬øC√≥mo llevar la autenticaci√≥n?
Cuando habilitas `proxy.auth_mode` y configuras `proxy.api_key`, las solicitudes deben llevar una API Key.

El middleware de Antigravity Tools lee primero `Authorization`, y tambi√©n es compatible con `x-api-key` y `x-goog-api-key`. (Ver implementaci√≥n en `src-tauri/src/proxy/middleware/auth.rs`)
:::

## ¬øQu√© es la API compatible con OpenAI?

La **API compatible con OpenAI** es un conjunto de rutas HTTP y protocolos JSON/SSE que "se parecen a OpenAI". El cliente env√≠a solicitudes al gateway local en el formato de OpenAI, el gateway convierte la solicitud en una llamada a upstream interna y convierte la respuesta de upstream de nuevo a la estructura de respuesta de OpenAI, permitiendo que el SDK de OpenAI existente se use casi sin modificaciones.

### Resumen de endpoints compatibles (relacionado con este tutorial)

| Endpoint | Prop√≥sito | Evidencia de c√≥digo |
| --- | --- | --- |
| `POST /v1/chat/completions` | Chat Completions (incluye streaming) | `src-tauri/src/proxy/server.rs` registro de ruta; `src-tauri/src/proxy/handlers/openai.rs` |
| `POST /v1/completions` | Legacy Completions (reutiliza el mismo manejador) | `src-tauri/src/proxy/server.rs` registro de ruta |
| `POST /v1/responses` | Compatibilidad con Responses/Codex CLI (reutiliza el mismo manejador) | `src-tauri/src/proxy/server.rs` registro de ruta (comentario: compatible con Codex CLI) |
| `GET /v1/models` | Devuelve la lista de modelos (incluye mapeo personalizado + generaci√≥n din√°mica) | `src-tauri/src/proxy/handlers/openai.rs` + `src-tauri/src/proxy/common/model_mapping.rs` |

## S√≠gueme paso a paso

### Paso 1: Confirma que el servicio est√° activo con curl (/healthz + /v1/models)

**Por qu√©**
Primero elimina problemas b√°sicos como "servicio no iniciado", "puerto incorrecto" o "bloqueado por firewall".

```bash
 # 1) Verificaci√≥n de salud
curl -s http://127.0.0.1:8045/healthz

 # 2) Obtener lista de modelos
curl -s http://127.0.0.1:8045/v1/models
```

**Deber√≠as ver**: `/healthz` devuelve algo como `{"status":"ok"}`; `/v1/models` devuelve `{"object":"list","data":[...]}`.

### Paso 2: Usa el SDK de OpenAI en Python para llamar a /v1/chat/completions

**Por qu√©**
Este paso demuestra que toda la cadena "SDK de OpenAI ‚Üí gateway local ‚Üí upstream ‚Üí conversi√≥n de respuesta de OpenAI" funciona.

```python
import openai

client = openai.OpenAI(
    api_key="sk-antigravity",
    base_url="http://127.0.0.1:8045/v1",
)

response = client.chat.completions.create(
    model="gemini-3-flash",
    messages=[{"role": "user", "content": "Hola, por favor pres√©ntate"}],
)

print(response.choices[0].message.content)
```

**Deber√≠as ver**: la terminal imprime un texto de respuesta del modelo.

### Paso 3: Activa stream y confirma el retorno de flujo SSE

**Por qu√©**
Muchos clientes dependen del protocolo SSE de OpenAI (`Content-Type: text/event-stream`). Este paso confirma que el flujo de streaming y el formato de eventos est√°n disponibles.

```bash
curl -N http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "stream": true,
    "messages": [
      {"role": "user", "content": "Explica en tres oraciones qu√© es un gateway de proxy inverso local"}
    ]
  }'
```

**Deber√≠as ver**: la terminal contin√∫a imprimiendo l√≠neas que comienzan con `data: { ... }` y termina con `data: [DONE]`.

### Paso 4: Ejecuta una solicitud con /v1/responses (estilo Codex/Responses)

**Por qu√©**
Algunas herramientas usan `/v1/responses` o usan campos como `instructions` e `input` en el cuerpo de la solicitud. Este proyecto "normaliza" este tipo de solicitudes a `messages` y luego reutiliza la misma l√≥gica de conversi√≥n. (Ver manejador en `src-tauri/src/proxy/handlers/openai.rs`)

```bash
curl -s http://127.0.0.1:8045/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "instructions": "Eres un revisor de c√≥digo riguroso.",
    "input": "Por favor se√±ala el bug m√°s probable en el siguiente c√≥digo:\n\nfunction add(a, b) { return a - b }"
  }'
```

**Deber√≠as ver**: el cuerpo de respuesta es un objeto de respuesta estilo OpenAI (este proyecto convierte la respuesta de Gemini a `choices[].message.content` de OpenAI).

### Paso 5: Confirma que el enrutamiento de modelos funciona (ve el encabezado de respuesta X-Mapped-Model)

**Por qu√©**
El `model` que escribes en el cliente no es necesariamente el "modelo f√≠sico" que se llama realmente. El gateway primero realiza el mapeo de modelos (incluye mapeo personalizado/comodines, ver [Enrutamiento de modelos: mapeo personalizado, prioridad de comodines y estrategias predefinidas](/es/lbjlaq/Antigravity-Manager/advanced/model-router/)), y pone el resultado final en los encabezados de respuesta para facilitar la soluci√≥n de problemas.

```bash
curl -i http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "hi"}]
  }'
```

**Deber√≠as ver**: los encabezados de respuesta contienen `X-Mapped-Model: ...` (por ejemplo, mapeado a `gemini-2.5-flash`), y tambi√©n pueden contener `X-Account-Email: ...`.

## Punto de control ‚úÖ

- `GET /healthz` devuelve `{"status":"ok"}` (o JSON equivalente)
- `GET /v1/models` devuelve `object=list` y `data` es un array
- Las solicitudes no streaming de `/v1/chat/completions` pueden obtener `choices[0].message.content`
- Cuando `stream: true`, se recibe SSE y termina con `[DONE]`
- `curl -i` puede ver el encabezado de respuesta `X-Mapped-Model`

## Advertencias de problemas comunes

### 1) Base URL incorrecta causa 404 (el m√°s com√∫n)

- En los ejemplos del SDK de OpenAI, `base_url` debe terminar con `/v1` (ver ejemplo de Python en README del proyecto).
- Algunos clientes "apilan rutas". Por ejemplo, README menciona expl√≠citamente: Kilo Code en modo OpenAI puede construir rutas no est√°ndar como `/v1/chat/completions/responses`, lo que activa 404.

### 2) 401: no es que upstream est√© ca√≠do, es que no llevaste key o el modo es incorrecto

Cuando el "modo efectivo" de la estrategia de autenticaci√≥n no es `off`, el middleware verifica los encabezados de solicitud: `Authorization: Bearer <proxy.api_key>`, y tambi√©n es compatible con `x-api-key` y `x-goog-api-key`. (Ver implementaci√≥n en `src-tauri/src/proxy/middleware/auth.rs`)

::: tip Sugerencia sobre modo de autenticaci√≥n
`auth_mode = auto` decidir√° autom√°ticamente seg√∫n `allow_lan_access`:
- `allow_lan_access = true` ‚Üí modo efectivo es `all_except_health` (todas las rutas excepto `/healthz` requieren autenticaci√≥n)
- `allow_lan_access = false` ‚Üí modo efectivo es `off` (acceso local no requiere autenticaci√≥n)
:::

### 3) 429/503/529: el proxy reintentar√° + rotar√° cuentas, pero tambi√©n puede "agotar el pool"

El manejador de OpenAI tiene incorporado hasta 3 intentos (y limitado por el tama√±o del pool de cuentas), y esperar√°/rotar√° cuentas para reintentar ante ciertos errores. (Ver implementaci√≥n en `src-tauri/src/proxy/handlers/openai.rs`)

## Resumen de este tutorial

- `/v1/chat/completions` es el punto de integraci√≥n m√°s universal, `stream: true` usar√° SSE
- `/v1/responses` y `/v1/completions` usan el mismo manejador compatible, el n√∫cleo es primero normalizar la solicitud a `messages`
- `X-Mapped-Model` te ayuda a confirmar el resultado del mapeo "nombre del modelo del cliente ‚Üí modelo f√≠sico final"

## Pr√≥ximo tutorial

> En el siguiente tutorial veremos **API compatible con Anthropic: contratos clave de /v1/messages y Claude Code** (cap√≠tulo correspondiente: `platforms-anthropic`).

---

## Ap√©ndice: Referencia de c√≥digo fuente

<details>
<summary><strong>Haz clic para expandir y ver ubicaci√≥n del c√≥digo fuente</strong></summary>

> √öltima actualizaci√≥n: 2026-01-23

| Funci√≥n | Ruta del archivo | N√∫mero de l√≠nea |
| --- | --- | --- |
| Registro de rutas OpenAI (incluye /v1/responses) | [`src-tauri/src/proxy/server.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/server.rs#L120-L194) | 120-194 |
| Manejador de Chat Completions (incluye detecci√≥n de formato Responses) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L70-L462) | 70-462 |
| Manejador de /v1/completions y /v1/responses (normalizaci√≥n Codex/Responses + reintentos/rotaci√≥n) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L464-L1080) | 464-1080 |
| Retorno de /v1/models (lista de modelos din√°mica) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L1082-L1102) | 1082-1102 |
| Estructura de datos de solicitud de OpenAI (messages/instructions/input/size/quality) | [`src-tauri/src/proxy/mappers/openai/models.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/models.rs#L7-L38) | 7-38 |
| Conversi√≥n de solicitud OpenAI ‚Üí Gemini (systemInstruction/thinkingConfig/tools) | [`src-tauri/src/proxy/mappers/openai/request.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/request.rs#L6-L553) | 6-553 |
| Conversi√≥n de respuesta Gemini ‚Üí OpenAI (choices/usageMetadata) | [`src-tauri/src/proxy/mappers/openai/response.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/mappers/openai/response.rs#L5-L214) | 5-214 |
| Mapeo de modelos y prioridad de comodines (exacto > comod√≠n > predeterminado) | [`src-tauri/src/proxy/common/model_mapping.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/common/model_mapping.rs#L180-L228) | 180-228 |
| Middleware de autenticaci√≥n (Authorization/x-api-key/x-goog-api-key) | [`src-tauri/src/proxy/middleware/auth.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/middleware/auth.rs#L15-L77) | 15-77 |

**Constantes clave**:
- `MAX_RETRY_ATTEMPTS = 3`: n√∫mero m√°ximo de intentos para el protocolo OpenAI (incluye rotaci√≥n) (ver `src-tauri/src/proxy/handlers/openai.rs`)

**Funciones clave**:
- `transform_openai_request(...)`: convierte el cuerpo de solicitud de OpenAI en solicitud a upstream interno (ver `src-tauri/src/proxy/mappers/openai/request.rs`)
- `transform_openai_response(...)`: convierte la respuesta de upstream en `choices`/`usage` de OpenAI (ver `src-tauri/src/proxy/mappers/openai/response.rs`)

</details>
