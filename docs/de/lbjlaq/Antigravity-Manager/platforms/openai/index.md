---
title: "OpenAI API: Einrichtung der Integration | Antigravity-Manager"
sidebarTitle: "In 5 Minuten das OpenAI SDK anschlie√üen"
subtitle: "OpenAI API: Einrichtung der Integration"
description: "Lerne die Einrichtung der OpenAI-kompatiblen API. Meistere Routing-Konvertierung, base_url-Einstellungen und die Fehlerbehebung von 401/404/429, um Antigravity Tools schnell zu nutzen."
tags:
  - "OpenAI"
  - "API-Proxy"
  - "Chat Completions"
  - "Responses"
  - "Antigravity Tools"
prerequisite:
  - "start-proxy-and-first-client"
  - "start-add-account"
order: 1
---

# OpenAI-kompatible API: Umsetzungsstrategien f√ºr /v1/chat/completions und /v1/responses

Du wirst diese **OpenAI-kompatible API** verwenden, um vorhandene OpenAI SDKs/Clients direkt mit dem lokalen Gateway von Antigravity Tools zu verbinden. Der Fokus liegt auf der erfolgreichen Ausf√ºhrung von `/v1/chat/completions` und `/v1/responses`, und du lernst, wie du mithilfe der Antwort-Header schnell Probleme beheben kannst.

## Was du nach diesem Tutorial kannst

- OpenAI SDKs (oder curl) direkt mit dem lokalen Gateway von Antigravity Tools verbinden
- `/v1/chat/completions` (inklusive `stream: true`) und `/v1/responses` erfolgreich ausf√ºhren
- Die Modellliste von `/v1/models` verstehen und den `X-Mapped-Model`-Header in den Antworten interpretieren
- Bei 401/404/429-Fehlern wissen, wo du zuerst suchen musst

## Dein aktuelles Problem

Viele Clients/SDKs erkennen nur das OpenAI-Interface-Format: feste URLs, feste JSON-Felder, festes SSE-Streaming-Format. Ziel von Antigravity Tools ist nicht, deinen Client zu √§ndern, sondern den Client glauben zu lassen, er w√ºrde OpenAI aufrufen ‚Äì w√§hrend die Anfragen tats√§chlich in interne Upstream-Aufrufe konvertiert und die Ergebnisse zur√ºck in das OpenAI-Format transformiert werden.

## Wann diese Methode verwenden

- Du hast bereits eine Reihe von Tools, die nur OpenAI unterst√ºtzen (IDE-Plugins, Skripte, Bots, SDKs) und m√∂chtest nicht f√ºr jedes eine neue Integration schreiben
- Du m√∂chtest √ºber eine einheitliche `base_url` Anfragen an das lokale (oder LAN-)Gateway senden und das Gateway dann die Kontoverwaltung, Wiederholungen und √úberwachung √ºbernehmen

## üéí Vorbereitungen vor dem Start

::: warning Voraussetzungen
- Du hast den Reverse-Proxy-Dienst auf der Seite "API Proxy" in Antigravity Tools gestartet und den Port notiert (z.B. `8045`)
- Du hast mindestens ein aktives Konto hinzugef√ºgt, da der Reverse-Proxy sonst kein Upstream-Token erhalten kann
:::

::: info Wie wird die Authentifizierung √ºbergeben?
Wenn du `proxy.auth_mode` aktiviert und `proxy.api_key` konfiguriert hast, m√ºssen Anfragen einen API-Schl√ºssel enthalten.

Die Middleware von Antigravity Tools liest zuerst `Authorization`, ist aber auch kompatibel mit `x-api-key` und `x-goog-api-key`. (Siehe Implementierung in `src-tauri/src/proxy/middleware/auth.rs`)
:::

## Was ist eine OpenAI-kompatible API?

Eine **OpenAI-kompatible API** ist eine Gruppe von HTTP-Routen und JSON/SSE-Protokollen, die "wie OpenAI aussehen". Clients senden Anfragen im OpenAI-Anfrageformat an das lokale Gateway, das Gateway wandelt die Anfragen in interne Upstream-Aufrufe um und konvertiert die Upstream-Antworten zur√ºck in die OpenAI-Antwortstruktur ‚Äì damit vorhandene OpenAI-SDKs fast ohne √Ñnderungen verwendet werden k√∂nnen.

### Schnell√ºbersicht der kompatiblen Endpunkte (relevant f√ºr dieses Tutorial)

| Endpunkt | Verwendung | Code-Nachweis |
|--- | --- | ---|
| `POST /v1/chat/completions` | Chat Completions (inkl. Streaming) | Routenregistrierung in `src-tauri/src/proxy/server.rs`; `src-tauri/src/proxy/handlers/openai.rs` |
| `POST /v1/completions` | Legacy Completions (wiederverwendet denselben Handler) | Routenregistrierung in `src-tauri/src/proxy/server.rs` |
| `POST /v1/responses` | Responses/Codex CLI-Kompatibilit√§t (wiederverwendet denselben Handler) | Routenregistrierung in `src-tauri/src/proxy/server.rs` (Kommentar: kompatibel mit Codex CLI) |
| `GET /v1/models` | Gibt Modellliste zur√ºck (inkl. benutzerdefinierte Zuordnung + dynamische Generierung) | `src-tauri/src/proxy/handlers/openai.rs` + `src-tauri/src/proxy/common/model_mapping.rs` |

## Folge mir

### Schritt 1: √úberpr√ºfe mit curl, ob der Dienst l√§uft (/healthz + /v1/models)

**Warum**
Erst grundlegende Probleme ausschlie√üen wie "Dienst nicht gestartet/Port falsch/durch Firewall blockiert".

```bash
 # 1) Gesundheitscheck
curl -s http://127.0.0.1:8045/healthz

 # 2) Modellliste abrufen
curl -s http://127.0.0.1:8045/v1/models
```

**Du solltest sehen**: `/healthz` gibt etwa `{"status":"ok"}` zur√ºck; `/v1/models` gibt `{"object":"list","data":[...]}` zur√ºck.

### Schritt 2: Rufe /v1/chat/completions mit dem OpenAI Python SDK auf

**Warum**
Dieser Schritt beweist, dass die gesamte Kette "OpenAI SDK ‚Üí lokales Gateway ‚Üí Upstream ‚Üí OpenAI-Antwortkonvertierung" funktioniert.

```python
import openai

client = openai.OpenAI(
    api_key="sk-antigravity",
    base_url="http://127.0.0.1:8045/v1",
)

response = client.chat.completions.create(
    model="gemini-3-flash",
    messages=[{"role": "user", "content": "Hallo, stelle dich bitte kurz vor"}],
)

print(response.choices[0].message.content)
```

**Du solltest sehen**: Das Terminal gibt einen Text mit der Modellantwort aus.

### Schritt 3: Aktiviere Streaming und best√§tige die SSE-Streaming-R√ºckgabe

**Warum**
Viele Clients verlassen sich auf das SSE-Protokoll von OpenAI (`Content-Type: text/event-stream`). Dieser Schritt best√§tigt, dass die Streaming-Verbindung und das Ereignisformat verf√ºgbar sind.

```bash
curl -N http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "stream": true,
    "messages": [
      {"role": "user", "content": "Erkl√§re in drei S√§tzen, was ein lokaler Reverse-Proxy-Gateway ist"}
    ]
  }'
```

**Du solltest sehen**: Das Terminal gibt kontinuierlich Zeilen aus, die mit `data: { ... }` beginnen und mit `data: [DONE]` enden.

### Schritt 4: F√ºhre eine Anfrage mit /v1/responses (Codex/Responses-Stil) erfolgreich aus

**Warum**
Einige Tools verwenden `/v1/responses` oder Felder wie `instructions` und `input` im Anfragek√∂rper. Dieses Projekt "normalisiert" solche Anfragen zu `messages` und verwendet dann dieselbe Konvertierungslogik. (Handler siehe `src-tauri/src/proxy/handlers/openai.rs`)

```bash
curl -s http://127.0.0.1:8045/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash",
    "instructions": "Du bist ein strenger Code-Pr√ºfer.",
    "input": "Bitte weise auf den wahrscheinlichsten Bug im folgenden Code hin:\n\nfunction add(a, b) { return a - b }"
  }'
```

**Du solltest sehen**: Die Antwort ist ein OpenAI-√§hnliches Antwortobjekt (dieses Projekt konvertiert Gemini-Antworten zu OpenAI `choices[].message.content`).

### Schritt 5: Best√§tige, dass das Modell-Routing funktioniert (siehe X-Mapped-Model Antwort-Header)

**Warum**
Das `model`, das du im Client schreibst, ist nicht unbedingt das tats√§chlich aufgerufene "physikalische Modell". Das Gateway f√ºhrt zuerst eine Modellzuordnung durch (inkl. benutzerdefinierte Zuordnung/Platzhalter, siehe [Modell-Routing: Benutzerdefinierte Zuordnung, Platzhalter-Priorit√§t und voreingestellte Strategien](/de/lbjlaq/Antigravity-Manager/advanced/model-router/)) und platziert das Endergebnis in den Antwort-Headern, damit du Fehler beheben kannst.

```bash
curl -i http://127.0.0.1:8045/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "hallo"}]
  }'
```

**Du solltest sehen**: Die Antwort-Header enthalten `X-Mapped-Model: ...` (z.B. zugeordnet zu `gemini-2.5-flash`) und m√∂glicherweise auch `X-Account-Email: ...`.

## Checkpoint ‚úÖ

- `GET /healthz` gibt `{"status":"ok"}` (oder √§quivalentes JSON) zur√ºck
- `GET /v1/models` gibt `object=list` zur√ºck und `data` ist ein Array
- Nicht-streaming-Anfragen an `/v1/chat/completions` erhalten `choices[0].message.content`
- Bei `stream: true` werden SSE-Nachrichten empfangen und mit `[DONE]` beendet
- `curl -i` zeigt den `X-Mapped-Model` Antwort-Header

## H√§ufige Fehler

### 1) Falsche Base URL f√ºhrt zu 404 (am h√§ufigsten)

- In OpenAI SDK-Beispielen muss `base_url` mit `/v1` enden (siehe Python-Beispiel im Projekt-README).
- Manche Clients "stapeln Pfade". Das README weist beispielsweise explizit darauf hin, dass Kilo Code im OpenAI-Modus m√∂glicherweise nicht-standardisierte Pfade wie `/v1/chat/completions/responses` bilden kann, was zu 404 f√ºhrt.

### 2) 401: Nicht der Downstream ist ausgefallen, sondern du hast keinen Key √ºbergeben oder der Modus ist falsch

Wenn der "aktive Modus" der Authentifizierungsstrategie nicht `off` ist, validiert die Middleware die Header: `Authorization: Bearer <proxy.api_key>`, ist aber auch kompatibel mit `x-api-key` und `x-goog-api-key`. (Siehe Implementierung in `src-tauri/src/proxy/middleware/auth.rs`)

::: tip Hinweis zum Authentifizierungsmodus
Bei `auth_mode = auto` wird basierend auf `allow_lan_access` automatisch entschieden:
- `allow_lan_access = true` ‚Üí aktiver Modus ist `all_except_health` (au√üer `/healthz` ist Authentifizierung erforderlich)
- `allow_lan_access = false` ‚Üí aktiver Modus ist `off` (lokaler Zugriff ohne Authentifizierung)
:::

### 3) 429/503/529: Der Proxy wiederholt und rotiert Konten, aber m√∂glicherweise ist der "Pool ersch√∂pft"

Der OpenAI-Handler f√ºhrt maximal 3 Versuche durch (begrenzt durch die Gr√∂√üe des Konten-Pools) und wartet/rotiert bei bestimmten Fehlern. (Siehe Implementierung in `src-tauri/src/proxy/handlers/openai.rs`)

## Zusammenfassung dieser Lektion

- `/v1/chat/completions` ist der universellste Endpunkt; `stream: true` verwendet SSE
- `/v1/responses` und `/v1/completions` verwenden denselben Kompatibilit√§ts-Handler; der Kern besteht darin, Anfragen zuerst zu `messages` zu normalisieren
- `X-Mapped-Model` hilft dir, das Zuordnungsergebnis "Client-Modellname ‚Üí endg√ºltiges physikalisches Modell" zu best√§tigen

## Vorschau auf die n√§chste Lektion

> In der n√§chsten Lektion werden wir uns **Anthropic-kompatible API: /v1/messages und Schl√ºsselkontrakte von Claude Code** ansehen (entsprechendes Kapitel: `platforms-anthropic`).

---

## Anhang: Quellcode-Referenz

<details>
<summary><strong>Klicke hier, um Quellcode-Positionen anzuzeigen</strong></summary>

> Aktualisiert am: 2026-01-23

| Funktion | Dateipfad | Zeilennummer |
|--- | --- | ---|
|--- | --- | ---|
|--- | --- | ---|
|--- | --- | ---|
| R√ºckgabe von /v1/models (dynamische Modellliste) | [`src-tauri/src/proxy/handlers/openai.rs`](https://github.com/lbjlaq/Antigravity-Manager/blob/main/src-tauri/src/proxy/handlers/openai.rs#L1082-L1102) | 1082-1102 |
|--- | --- | ---|
|--- | --- | ---|
|--- | --- | ---|
|--- | --- | ---|
|--- | --- | ---|

**Wichtige Konstanten**:
- `MAX_RETRY_ATTEMPTS = 3`: Maximale Anzahl von Versuchen f√ºr das OpenAI-Protokoll (inkl. Rotation) (siehe `src-tauri/src/proxy/handlers/openai.rs`)

**Wichtige Funktionen**:
- `transform_openai_request(...)`: Konvertiert OpenAI-Anfragek√∂rper in interne Upstream-Anfrage (siehe `src-tauri/src/proxy/mappers/openai/request.rs`)
- `transform_openai_response(...)`: Konvertiert Upstream-Antworten zu OpenAI `choices`/`usage` (siehe `src-tauri/src/proxy/mappers/openai/response.rs`)

</details>
