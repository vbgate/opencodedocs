---
title: "Provider-Einrichtung: Multi-Modell-Strategie | oh-my-opencode"
sidebarTitle: "Mehrere KI-Dienste verbinden"
subtitle: "Provider-Einrichtung: Multi-Modell-Strategie"
description: "Lernen Sie, wie Sie verschiedene KI-Provider f√ºr oh-my-opencode konfigurieren, einschlie√ülich Anthropic, OpenAI, Google und GitHub Copilot, sowie wie die automatische Modell-Degradierung funktioniert."
tags:
  - "configuration"
  - "providers"
  - "models"
prerequisite:
  - "start-installation"
order: 40
---

# Provider-Einrichtung: Claude, OpenAI, Gemini und Multi-Modell-Strategie

## Was Sie nach diesem Tutorial k√∂nnen werden

- Konfigurieren Sie mehrere KI-Provider wie Anthropic Claude, OpenAI, Google Gemini und GitHub Copilot
- Verstehen Sie den Multi-Modell-Priorit√§ts-Degradierungsmechanismus, der das System automatisch das beste verf√ºgbare Modell ausw√§hlen l√§sst
- Weisen Sie die am besten geeigneten Modelle f√ºr verschiedene KI-Agenten und Aufgabentypen zu
- Konfigurieren Sie Drittanbieter-Dienste wie Z.ai Coding Plan und OpenCode Zen
- Verwenden Sie den Doctor-Befehl zur Diagnose der Modellaufl√∂sungskonfiguration

## Ihre aktuelle Herausforderung

Sie haben oh-my-opencode installiert, aber sind sich nicht sicher:
- Wie Sie mehrere KI-Provider hinzuf√ºgen (Claude, OpenAI, Gemini usw.)
- Warum ein Agent manchmal nicht das erwartete Modell verwendet
- Wie Sie verschiedene Modelle f√ºr verschiedene Aufgaben konfigurieren (z. B. g√ºnstige f√ºr Recherche, leistungsstarke f√ºr Programmierung)
- Wie das System automatisch auf ein Backup-Modell umschaltet, wenn ein Provider nicht verf√ºgbar ist
- Wie die Modellkonfiguration in `opencode.json` und `oh-my-opencode.json` zusammenarbeitet

## Wann diese Technik verwenden

- **Erstkonfiguration**: Gerade oh-my-opencode installiert und m√ºssen KI-Provider hinzuf√ºgen oder anpassen
- **Neues Abonnement hinzuf√ºgen**: Ein neues KI-Service-Abonnement gekauft (z. B. Gemini Pro) und m√∂chten es integrieren
- **Kosten optimieren**: M√∂chten, dass bestimmte Agenten g√ºnstigere oder schnellere Modelle verwenden
- **Fehlerbehebung**: Festgestellt, dass ein Agent nicht das erwartete Modell verwendet, und m√ºssen das Problem diagnostizieren
- **Multi-Modell-Orchestrierung**: M√∂chten die Vorteile verschiedener Modelle nutzen, um intelligente Entwicklungsworkflows zu erstellen

## üéí Vorbereitung vor dem Start

::: warning Voraussetzungspr√ºfung
Dieses Tutorial setzt voraus, dass Sie:
- ‚úÖ Die [Installation und Erstkonfiguration](../installation/) abgeschlossen haben
- ‚úÖ OpenCode installiert haben (Version >= 1.0.150)
- ‚úÖ Grundlegende JSON/JSONC-Konfigurationsdateiformate verstehen
:::

## Kernkonzept

oh-my-opencode verwendet ein **Multi-Modell-Orchestrierungssystem**, das basierend auf Ihren Abonnements und Konfigurationen die am besten geeigneten Modelle f√ºr verschiedene KI-Agenten und Aufgabentypen ausw√§hlt.

**Warum mehrere Modelle ben√∂tigt werden?**

Verschiedene Modelle haben unterschiedliche St√§rken:
- **Claude Opus 4.5**: Stark in komplexem Reasoning und Architekturdesign (hohe Kosten, aber hohe Qualit√§t)
- **GPT-5.2**: Stark in Code-Debugging und strategischer Beratung
- **Gemini 3 Pro**: Stark in Frontend- und UI/UX-Aufgaben (starke visuelle F√§higkeiten)
- **GPT-5 Nano**: Schnell und kostenlos, geeignet f√ºr Code-Suche und einfache Erkundung
- **GLM-4.7**: Hohe Kosten-Nutzen-Verh√§ltnis, geeignet f√ºr Recherche und Dokumentensuche

Die Intelligenz von oh-my-opencode liegt darin: **Jede Aufgabe verwendet das am besten geeignete Modell, anstatt alle Aufgaben mit demselben Modell auszuf√ºhren**.

## Konfigurationsdateispeicherort

oh-my-opencode unterst√ºtzt zwei Konfigurationsebenen:

| Speicherort | Pfad | Priorit√§t | Anwendungsszenario |
|---|---|---|---|
| **Projektkonfiguration** | `.opencode/oh-my-opencode.json` | Niedrig | Projektspezifische Konfiguration (mit Codebasis committet) |
| **Benutzerkonfiguration** | `~/.config/opencode/oh-my-opencode.json` | Hoch | Globale Konfiguration (von allen Projekten gemeinsam genutzt) |

**Konfigurationszusammenf√ºhrungsregel**: Benutzerkonfiguration √ºberschreibt Projektkonfiguration.

**Empfohlene Konfigurationsdateistruktur**:

```jsonc
{
  "$schema": "https://raw.githubusercontent.com/code-yeongyu/oh-my-opencode/master/assets/oh-my-opencode.schema.json",
  // JSON Schema-Autovervollst√§ndigung aktivieren

  "agents": {
    // Agent-Modell-Overrides
  },
  "categories": {
    // Kategorie-Modell-Overrides
  }
}
```

::: tip Schema-Autovervollst√§ndigung
In Editoren wie VS Code erhalten Sie nach dem Hinzuf√ºgen des `$schema`-Felds vollst√§ndige Autovervollst√§ndigung und Typpr√ºfung w√§hrend der Eingabe.
:::

## Provider-Konfigurationsmethoden

oh-my-opencode unterst√ºtzt 6 Haupt-Provider. Die Konfigurationsmethoden variieren je nach Provider.

### Anthropic Claude (Empfohlen)

**Anwendungsszenario**: Hauptorchestrator Sisyphus und die meisten Kern-Agenten

**Konfigurationsschritte**:

1. **OpenCode-Authentifizierung ausf√ºhren**:
   ```bash
   opencode auth login
   ```

2. **Provider ausw√§hlen**:
   - `Provider`: W√§hlen Sie `Anthropic`
   - `Login method`: W√§hlen Sie `Claude Pro/Max`

3. **OAuth-Flow abschlie√üen**:
   - Das System √∂ffnet automatisch den Browser
   - Melden Sie sich bei Ihrem Claude-Konto an
   - Warten Sie auf die Fertigstellung der Authentifizierung

4. **Erfolg verifizieren**:
   ```bash
   opencode models | grep anthropic
   ```

   Sie sollten sehen:
   - `anthropic/claude-opus-4-5`
   - `anthropic/claude-sonnet-4-5`
   - `anthropic/claude-haiku-4-5`

**Modellzuordnung** (Sisyphus-Standardkonfiguration):

| Agent | Standardmodell | Verwendung |
|---|---|---|
| Sisyphus | `anthropic/claude-opus-4-5` | Hauptorchestrator, komplexes Reasoning |
| Prometheus | `anthropic/claude-opus-4-5` | Projektplanung |
| Metis | `anthropic/claude-sonnet-4-5` | Vorplanungsanalyse |
| Momus | `anthropic/claude-opus-4-5` | Planungs√ºberpr√ºfung |

### OpenAI (ChatGPT Plus)

**Anwendungsszenario**: Oracle-Agent (Architektur√ºberpr√ºfung, Debugging)

**Konfigurationsschritte**:

1. **OpenCode-Authentifizierung ausf√ºhren**:
   ```bash
   opencode auth login
   ```

2. **Provider ausw√§hlen**:
   - `Provider`: W√§hlen Sie `OpenAI`
   - `Login method`: W√§hlen Sie OAuth oder API Key

3. **Authentifizierungsflow abschlie√üen** (je nach gew√§hlter Methode)

4. **Erfolg verifizieren**:
   ```bash
   opencode models | grep openai
   ```

**Modellzuordnung** (Oracle-Standardkonfiguration):

| Agent | Standardmodell | Verwendung |
|---|---|---|
| Oracle | `openai/gpt-5.2` | Architektur√ºberpr√ºfung, Debugging |

**Manuelles √úberschreibungsbeispiel**:

```jsonc
{
  "agents": {
    "oracle": {
      "model": "openai/gpt-5.2",  // Verwendet GPT f√ºr strategisches Reasoning
      "temperature": 0.1
    }
  }
}
```

### Google Gemini (Empfohlen)

**Anwendungsszenario**: Multimodal Looker (Medienanalyse), Frontend UI/UX-Aufgaben

::: tip Sehr empfohlen
F√ºr die Gemini-Authentifizierung wird dringend empfohlen, das Plugin [`opencode-antigravity-auth`](https://github.com/NoeFabris/opencode-antigravity-auth) zu installieren. Es bietet:
- Multi-Account-Lastenausgleich (bis zu 10 Konten)
- Variant-System-Unterst√ºtzung (`low`/`high`-Varianten)
- Duales Kontingentsystem (Antigravity + Gemini CLI)
:::

**Konfigurationsschritte**:

1. **Antigravity-Authentifizierungs-Plugin hinzuf√ºgen**:
   
   Bearbeiten Sie `~/.config/opencode/opencode.json`:
   ```json
   {
     "plugin": [
       "oh-my-opencode",
       "opencode-antigravity-auth@latest"
     ]
   }
   ```

2. **Gemini-Modelle konfigurieren** (wichtig):
   
   Das Antigravity-Plugin verwendet unterschiedliche Modellnamen. Sie m√ºssen die vollst√§ndige Modellkonfiguration nach `opencode.json` kopieren, wobei Sie sorgf√§ltig zusammenf√ºhren m√ºssen, um bestehende Einstellungen nicht zu besch√§digen.

   Verf√ºgbare Modelle (Antigravity-Kontingent):
   - `google/antigravity-gemini-3-pro` ‚Äî Varianten: `low`, `high`
   - `google/antigravity-gemini-3-flash` ‚Äî Varianten: `minimal`, `low`, `medium`, `high`
   - `google/antigravity-claude-sonnet-4-5` ‚Äî keine Varianten
   - `google/antigravity-claude-sonnet-4-5-thinking` ‚Äî Varianten: `low`, `max`
   - `google/antigravity-claude-opus-4-5-thinking` ‚Äî Varianten: `low`, `max`

   Verf√ºgbare Modelle (Gemini CLI-Kontingent):
   - `google/gemini-2.5-flash`, `google/gemini-2.5-pro`, `google/gemini-3-flash-preview`, `google/gemini-3-pro-preview`

3. **Agent-Modell √ºberschreiben** (in `oh-my-opencode.json`):
   
   ```jsonc
   {
     "agents": {
       "multimodal-looker": {
         "model": "google/antigravity-gemini-3-flash"
       }
     }
   }
   ```

4. **Authentifizierung ausf√ºhren**:
   ```bash
   opencode auth login
   ```

5. **Provider ausw√§hlen**:
   - `Provider`: W√§hlen Sie `Google`
   - `Login method`: W√§hlen Sie `OAuth with Google (Antigravity)`

6. **Authentifizierungsflow abschlie√üen**:
   - Das System √∂ffnet automatisch den Browser
   - Melden Sie sich bei Google an
   - Optional: F√ºgen Sie weitere Google-Konten f√ºr Lastenausgleich hinzu

**Modellzuordnung** (Standardkonfiguration):

| Agent | Standardmodell | Verwendung |
|---|---|---|
| Multimodal Looker | `google/antigravity-gemini-3-flash` | PDF-, Bildanalyse |

### GitHub Copilot (Backup-Provider)

**Anwendungsszenario**: Alternative Option, wenn native Provider nicht verf√ºgbar sind

::: info Backup-Provider
GitHub Copilot fungiert als Proxy-Provider, der Anfragen an das/die von Ihnen abonnierte(n) zugrunde liegende(n) Modell(e) weiterleitet.
:::

**Konfigurationsschritte**:

1. **OpenCode-Authentifizierung ausf√ºhren**:
   ```bash
   opencode auth login
   ```

2. **Provider ausw√§hlen**:
   - `Provider`: W√§hlen Sie `GitHub`
   - `Login method`: W√§hlen Sie `Authenticate via OAuth`

3. **GitHub OAuth-Flow abschlie√üen**

4. **Erfolg verifizieren**:
   ```bash
   opencode models | grep github-copilot
   ```

**Modellzuordnung** (wenn GitHub Copilot der bestverf√ºgbare Provider ist):

| Agent | Modell | Verwendung |
|---|---|---|
| Sisyphus | `github-copilot/claude-opus-4.5` | Hauptorchestrator |
| Oracle | `github-copilot/gpt-5.2` | Architektur√ºberpr√ºfung |
| Explore | `opencode/gpt-5-nano` | Schnelle Erkundung |
| Librarian | `zai-coding-plan/glm-4.7` (wenn Z.ai verf√ºgbar) | Dokumentensuche |

### Z.ai Coding Plan (Optional)

**Anwendungsszenario**: Librarian-Agent (Multi-Repository-Recherche, Dokumentensuche)

**Merkmale**:
- Bietet GLM-4.7-Modell
- Hohe Kosten-Nutzen-Verh√§ltnis
- Wenn aktiviert, **verwendet der Librarian-Agent immer** `zai-coding-plan/glm-4.7`, unabh√§ngig von anderen verf√ºgbaren Providern

**Konfigurationsschritte**:

Verwenden Sie den interaktiven Installer:

```bash
bunx oh-my-opencode install
# Wenn gefragt: "Do you have a Z.ai Coding Plan subscription?" ‚Üí W√§hlen Sie "Yes"
```

**Modellzuordnung** (wenn Z.ai der einzige verf√ºgbare Provider ist):

| Agent | Modell | Verwendung |
|---|---|---|
| Sisyphus | `zai-coding-plan/glm-4.7` | Hauptorchestrator |
| Oracle | `zai-coding-plan/glm-4.7` | Architektur√ºberpr√ºfung |
| Explore | `zai-coding-plan/glm-4.7-flash` | Schnelle Erkundung |
| Librarian | `zai-coding-plan/glm-4.7` | Dokumentensuche |

### OpenCode Zen (Optional)

**Anwendungsszenario**: Bietet `opencode/`-pr√§fix-Modelle (Claude Opus 4.5, GPT-5.2, GPT-5 Nano, Big Pickle)

**Konfigurationsschritte**:

```bash
bunx oh-my-opencode install
# Wenn gefragt: "Do you have access to OpenCode Zen (opencode/ models)?" ‚Üí W√§hlen Sie "Yes"
```

**Modellzuordnung** (wenn OpenCode Zen der bestverf√ºgbare Provider ist):

| Agent | Modell | Verwendung |
|---|---|---|
| Sisyphus | `opencode/claude-opus-4-5` | Hauptorchestrator |
| Oracle | `opencode/gpt-5.2` | Architektur√ºberpr√ºfung |
| Explore | `opencode/gpt-5-nano` | Schnelle Erkundung |
| Librarian | `opencode/big-pickle` | Dokumentensuche |

## Modellaufl√∂sungssystem (3-Schritt-Priorit√§t)

oh-my-opencode verwendet ein **3-Schritt-Priorit√§tsmechanismus**, um das f√ºr jeden Agenten und jede Kategorie verwendete Modell zu bestimmen. Dieser Mechanismus stellt sicher, dass das System immer ein verf√ºgbares Modell findet.

### Schritt 1: Benutzer√ºberschreibung

Wenn der Benutzer ein Modell explizit in `oh-my-opencode.json` angibt, wird dieses Modell verwendet.

**Beispiel**:
```jsonc
{
  "agents": {
    "oracle": {
      "model": "openai/gpt-5.2"  // Benutzer explizit angegeben
    }
  }
}
```

In diesem Fall:
- ‚úÖ Verwendet direkt `openai/gpt-5.2`
- ‚ùå √úberspringt den Provider-Degradierungsschritt

### Schritt 2: Provider-Degradierung

Wenn der Benutzer kein Modell explizit angibt, versucht das System nacheinander die in der Priorit√§tskette des Agenten definierten Provider, bis ein verf√ºgbares Modell gefunden wird.

**Sisyphus' Provider-Priorit√§tskette**:

```
anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google
```

**Aufl√∂sungsprozess**:
1. Versucht `anthropic/claude-opus-4-5`
   - Verf√ºgbar? ‚Üí Gibt dieses Modell zur√ºck
   - Nicht verf√ºgbar? ‚Üí F√§hrt mit n√§chstem Schritt fort
2. Versucht `github-copilot/claude-opus-4-5`
   - Verf√ºgbar? ‚Üí Gibt dieses Modell zur√ºck
   - Nicht verf√ºgbar? ‚Üí F√§hrt mit n√§chstem Schritt fort
3. Versucht `opencode/claude-opus-4-5`
   - ...
4. Versucht `google/antigravity-claude-opus-4-5-thinking` (wenn konfiguriert)
   - ...
5. Gibt das Systemstandardmodell zur√ºck

**Provider-Priorit√§tsketten aller Agenten**:

| Agent | Modell (ohne Pr√§fix) | Provider-Priorit√§tskette |
|---|---|---|
| **Sisyphus** | `claude-opus-4-5` | anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google |
| **Oracle** | `gpt-5.2` | openai ‚Üí anthropic ‚Üí google ‚Üí github-copilot ‚Üí opencode |
| **Librarian** | `big-pickle` | opencode ‚Üí github-copilot ‚Üí anthropic |
| **Explore** | `gpt-5-nano` | anthropic ‚Üí opencode |
| **Multimodal Looker** | `gemini-3-flash` | google ‚Üí openai ‚Üí zai-coding-plan ‚Üí anthropic ‚Üí opencode |
| **Prometheus** | `claude-opus-4-5` | anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google |
| **Metis** | `claude-sonnet-4-5` | anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google |
| **Momus** | `claude-opus-4-5` | anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google |
| **Atlas** | `claude-sonnet-4-5` | anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google |

**Provider-Priorit√§tsketten von Kategorien**:

| Kategorie | Modell (ohne Pr√§fix) | Provider-Priorit√§tskette |
|---|---|---|
| **ultrabrain** | `gpt-5.2-codex` | openai ‚Üí anthropic ‚Üí google ‚Üí github-copilot ‚Üí opencode |
| **artistry** | `gemini-3-pro` | google ‚Üí openai ‚Üí anthropic ‚Üí github-copilot ‚Üí opencode |
| **quick** | `claude-haiku-4-5` | anthropic ‚Üí github-copilot ‚Üí opencode ‚Üí antigravity ‚Üí google |
| **writing** | `gemini-3-flash` | google ‚Üí openai ‚Üí anthropic ‚Üí github-copilot ‚Üí opencode |

### Schritt 3: Systemstandard

Wenn alle Provider nicht verf√ºgbar sind, wird das OpenCode-Standardmodell verwendet (aus `opencode.json` gelesen).

**Globale Priorit√§tsreihenfolge**:

```
Benutzer√ºberschreibung > Provider-Degradierung > Systemstandard
```

## Schritt-f√ºr-Schritt: Konfiguration mehrerer Provider

### Schritt 1: Planen Sie Ihre Abonnements

Bevor Sie mit der Konfiguration beginnen, organisieren Sie Ihre Abonnements:

```markdown
- [ ] Anthropic Claude (Pro/Max)
- [ ] OpenAI ChatGPT Plus
- [ ] Google Gemini
- [ ] GitHub Copilot
- [ ] Z.ai Coding Plan
- [ ] OpenCode Zen
```

### Schritt 2: Verwenden Sie den interaktiven Installer (empfohlen)

oh-my-opencode bietet einen interaktiven Installer, der die meisten Konfigurationen automatisch durchf√ºhrt:

```bash
bunx oh-my-opencode install
```

Der Installer fragt nach:
1. **Do you have a Claude Pro/Max Subscription?**
   - `yes, max20` ‚Üí `--claude=max20`
   - `yes, regular` ‚Üí `--claude=yes`
   - `no` ‚Üí `--claude=no`

2. **Do you have an OpenAI/ChatGPT Plus Subscription?**
   - `yes` ‚Üí `--openai=yes`
   - `no` ‚Üí `--openai=no`

3. **Will you integrate Gemini models?**
   - `yes` ‚Üí `--gemini=yes`
   - `no` ‚Üí `--gemini=no`

4. **Do you have a GitHub Copilot Subscription?**
   - `yes` ‚Üí `--copilot=yes`
   - `no` ‚Üí `--copilot=no`

5. **Do you have access to OpenCode Zen (opencode/ models)?**
   - `yes` ‚Üí `--opencode-zen=yes`
   - `no` ‚Üí `--opencode-zen=no`

6. **Do you have a Z.ai Coding Plan subscription?**
   - `yes` ‚Üí `--zai-coding-plan=yes`
   - `no` ‚Üí `--zai-coding-plan=no`

**Nicht-interaktiver Modus** (geeignet f√ºr Skriptinstallation):

```bash
bunx oh-my-opencode install --no-tui \
  --claude=max20 \
  --openai=yes \
  --gemini=yes \
  --copilot=no
```

### Schritt 3: Authentifizieren Sie jeden Provider

Nach Abschluss des Installers authentifizieren Sie nacheinander:

```bash
# Anthropic authentifizieren
opencode auth login
# Provider: Anthropic
# Login method: Claude Pro/Max
# OAuth-Flow abschlie√üen

# OpenAI authentifizieren
opencode auth login
# Provider: OpenAI
# OAuth-Flow abschlie√üen

# Google Gemini authentifizieren (Antigravity-Plugin zuerst installieren)
opencode auth login
# Provider: Google
# Login method: OAuth with Google (Antigravity)
# OAuth-Flow abschlie√üen

# GitHub Copilot authentifizieren
opencode auth login
# Provider: GitHub
# Login method: Authenticate via OAuth
# GitHub OAuth abschlie√üen
```

### Schritt 4: Konfiguration verifizieren

```bash
# OpenCode-Version pr√ºfen
opencode --version
# Sollte >= 1.0.150 sein

# Alle verf√ºgbaren Modelle anzeigen
opencode models

# Diagnose ausf√ºhren
bunx oh-my-opencode doctor --verbose
```

**Sie sollten sehen** (Doctor-Ausgabebeispiel):

```
‚úÖ OpenCode version: 1.0.150
‚úÖ Plugin loaded: oh-my-opencode

üìä Model Resolution:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Agent           ‚îÇ Requirement            ‚îÇ Resolved         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Sisyphus        ‚îÇ anthropic/claude-opus-4-5  ‚îÇ anthropic/claude-opus-4-5 ‚îÇ
‚îÇ Oracle           ‚îÇ openai/gpt-5.2              ‚îÇ openai/gpt-5.2              ‚îÇ
‚îÇ Librarian        ‚îÇ opencode/big-pickle           ‚îÇ opencode/big-pickle           ‚îÇ
‚îÇ Explore          ‚îÇ anthropic/gpt-5-nano          ‚îÇ anthropic/gpt-5-nano          ‚îÇ
‚îÇ Multimodal Looker‚îÇ google/gemini-3-flash          ‚îÇ google/gemini-3-flash          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚úÖ All models resolved successfully
```

### Schritt 5: Agent-Modelle anpassen (optional)

Wenn Sie f√ºr einen bestimmten Agenten ein anderes Modell angeben m√∂chten:

```jsonc
{
  "$schema": "https://raw.githubusercontent.com/code-yeongyu/oh-my-opencode/master/assets/oh-my-opencode.schema.json",

  "agents": {
    // Oracle verwendet GPT f√ºr Architektur√ºberpr√ºfung
    "oracle": {
      "model": "openai/gpt-5.2",
      "temperature": 0.1
    },

    // Librarian verwendet g√ºnstigeres Modell f√ºr Recherche
    "librarian": {
      "model": "opencode/gpt-5-nano",
      "temperature": 0.1
    },

    // Multimodal Looker verwendet Antigravity Gemini
    "multimodal-looker": {
      "model": "google/antigravity-gemini-3-flash",
      "variant": "high"
    }
  }
}
```

### Schritt 6: Kategorie-Modelle anpassen (optional)

Geben Sie Modelle f√ºr verschiedene Aufgabentypen an:

```jsonc
{
  "$schema": "https://raw.githubusercontent.com/code-yeongyu/oh-my-opencode/master/assets/oh-my-opencode.schema.json",

  "categories": {
    // Schnelle Aufgaben verwenden g√ºnstiges Modell
    "quick": {
      "model": "opencode/gpt-5-nano",
      "temperature": 0.1
    },

    // Frontend-Aufgaben verwenden Gemini
    "visual-engineering": {
      "model": "google/gemini-3-pro",
      "temperature": 0.7,
      "prompt_append": "Use shadcn/ui components and Tailwind CSS."
    },

    // Hochintelligente Reasoning-Aufgaben verwenden GPT Codex
    "ultrabrain": {
      "model": "openai/gpt-5.2-codex",
      "temperature": 0.1
    }
  }
}
```

**Kategorie verwenden**:

```markdown
// Verwendung von delegate_task im Dialog
delegate_task(category="visual", prompt="Create a responsive dashboard component")
delegate_task(category="quick", skills=["git-master"], prompt="Commit these changes")
```

## Pr√ºfpunkte ‚úÖ

- [ ] `opencode --version` zeigt Version >= 1.0.150 an
- [ ] `opencode models` listet Modelle aller konfigurierten Provider auf
- [ ] `bunx oh-my-opencode doctor --verbose` zeigt, dass alle Agentenmodelle korrekt aufgel√∂st sind
- [ ] In `opencode.json` ist `"oh-my-opencode"` im `plugin`-Array zu sehen
- [ ] Versuchen Sie, einen Agenten (z. B. Sisyphus) zu verwenden, um zu best√§tigen, dass das Modell funktioniert

## Warnhinweise

### ‚ùå Falle 1: Provider-Authentifizierung vergessen

**Symptom**: Provider konfiguriert, aber Modellaufl√∂sung schl√§gt fehl.

**Ursache**: Der Installer hat die Modelle konfiguriert, aber die Authentifizierung wurde nicht abgeschlossen.

**L√∂sung**:
```bash
opencode auth login
# W√§hlen Sie den entsprechenden Provider und schlie√üen Sie die Authentifizierung ab
```

### ‚ùå Falle 2: Falsche Antigravity-Modellnamen

**Symptom**: Gemini konfiguriert, aber der Agent verwendet es nicht.

**Ursache**: Das Antigravity-Plugin verwendet unterschiedliche Modellnamen (`google/antigravity-gemini-3-pro` statt `google/gemini-3-pro`).

**L√∂sung**:
```jsonc
{
  "agents": {
    "multimodal-looker": {
      "model": "google/antigravity-gemini-3-flash"  // Korrekt
      // model: "google/gemini-3-flash"  // ‚ùå Falsch
    }
  }
}
```

### ‚ùå Falle 3: Falsche Konfigurationsdateispeicherort

**Symptom**: Konfiguration ge√§ndert, aber das System hat sie nicht √ºbernommen.

**Ursache**: Falscher Konfigurationsdateispeicherort (Benutzerkonfiguration vs. Projektkonfiguration).

**L√∂sung**:
```bash
# Benutzerkonfiguration (global, hohe Priorit√§t)
~/.config/opencode/oh-my-opencode.json

# Projektkonfiguration (lokal, niedrige Priorit√§t)
.opencode/oh-my-opencode.json

# √úberpr√ºfen, welche Datei verwendet wird
bunx oh-my-opencode doctor --verbose
```

### ‚ùå Falle 4: Provider-Priorit√§tskette unterbrochen

**Symptom**: Ein Agent verwendet immer das falsche Modell.

**Ursache**: Die Benutzer√ºberschreibung (Schritt 1) √ºberspringt die Provider-Degradierung (Schritt 2) vollst√§ndig.

**L√∂sung**: Wenn Sie die automatische Degradierung nutzen m√∂chten, kodieren Sie das Modell nicht hart in `oh-my-opencode.json`, sondern lassen Sie das System basierend auf der Priorit√§tskette automatisch ausw√§hlen.

**Beispiel**:
```jsonc
{
  "agents": {
    "oracle": {
      // ‚ùå Hartkodiert: Verwendet immer GPT, auch wenn Anthropic verf√ºgbar ist
      "model": "openai/gpt-5.2"
    }
  }
}
```

Um die Degradierung zu nutzen, entfernen Sie das `model`-Feld und lassen Sie das System automatisch ausw√§hlen:
```jsonc
{
  "agents": {
    "oracle": {
      // ‚úÖ Automatisch: anthropic ‚Üí google ‚Üí github-copilot ‚Üí opencode
      "temperature": 0.1
    }
  }
}
```

### ‚ùå Falle 5: Z.ai belegt immer den Librarian

**Symptom**: Auch wenn andere Provider konfiguriert sind, verwendet Librarian immer noch GLM-4.7.

**Ursache**: Wenn Z.ai aktiviert ist, ist der Librarian hartkodiert, um `zai-coding-plan/glm-4.7` zu verwenden.

**L√∂sung**: Wenn Sie dieses Verhalten nicht ben√∂tigen, deaktivieren Sie Z.ai:
```bash
bunx oh-my-opencode install --no-tui --zai-coding-plan=no
```

Oder manuelles √úberschreiben:
```jsonc
{
  "agents": {
    "librarian": {
      "model": "opencode/big-pickle"  // √úberschreibt die Z.ai-Hartkodierung
    }
  }
}
```

## Lektionszusammenfassung

- oh-my-opencode unterst√ºtzt 6 Haupt-Provider: Anthropic, OpenAI, Google, GitHub Copilot, Z.ai, OpenCode Zen
- Verwenden Sie den interaktiven Installer `bunx oh-my-opencode install`, um mehrere Provider schnell zu konfigurieren
- Das Modellaufl√∂sungssystem w√§hlt dynamisch Modelle durch 3 Schritte Priorit√§t (Benutzer√ºberschreibung ‚Üí Provider-Degradierung ‚Üí Systemstandard)
- Jeder Agent und jede Kategorie hat seine eigene Provider-Priorit√§tskette, um sicherzustellen, dass immer ein verf√ºgbares Modell gefunden wird
- Verwenden Sie den Befehl `doctor --verbose`, um die Modellaufl√∂sungskonfiguration zu diagnostizieren
- Beim Anpassen von Agent- und Kategorie-Modellen m√ºssen Sie vorsichtig sein, den automatischen Degradierungsmechanismus nicht zu unterbrechen

## Vorschau auf die n√§chste Lektion

> In der n√§chsten Lektion lernen wir **[Multi-Modell-Strategie: Automatische Degradierung und Priorit√§ten](../model-resolution/)**.
>
> Sie werden lernen:
> - Den vollst√§ndigen Arbeitsablauf des Modellaufl√∂sungssystems
> - Wie man optimale Modellkombinationen f√ºr verschiedene Aufgaben entwirft
> - Strategien zur Parallelit√§tskontrolle in Hintergrundaufgaben
> - Wie man Modellaufl√∂sungsprobleme diagnostiziert

---

## Anhang: Quellcode-Referenz

<details>
<summary><strong>Klicken Sie, um die Quellcode-Position anzuzeigen</strong></summary>

> Aktualisierungszeit: 2026-01-26

| Funktion | Dateipfad | Zeilennummer |
|---|---|---|
| Konfiguration Schema-Definition | [`src/config/schema.ts`](https://github.com/code-yeongyu/oh-my-opencode/blob/main/src/config/schema.ts) | 1-378 |
| Installationsanleitung (Provider-Konfiguration) | [`docs/guide/installation.md`](https://github.com/code-yeongyu/oh-my-opencode/blob/main/docs/guide/installation.md) | 1-299 |
| Konfigurationsreferenz (Modellaufl√∂sung) | [`docs/configurations.md`](https://github.com/code-yeongyu/oh-my-opencode/blob/main/docs/configurations.md) | 391-512 |
| Agent-Override-Konfiguration Schema | [`src/config/schema.ts:AgentOverrideConfigSchema`](https://github.com/code-yeongyu/oh-my-opencode/blob/main/src/config/schema.ts#L98-L119) | 98-119 |
| Kategorie-Konfiguration Schema | [`src/config/schema.ts:CategoryConfigSchema`](https://github.com/code-yeongyu/oh-my-opencode/blob/main/src/config/schema.ts#L154-L172) | 154-172 |
| Provider-Priorit√§tskette-Dokumentation | [`docs/configurations.md`](https://github.com/code-yeongyu/oh-my-opencode/blob/main/docs/configurations.md#L445-L473) | 445-473 |

**Wichtige Konstanten**:
- Keine: Provider-Priorit√§tskette ist in der Konfigurationsdokumentation hartkodiert, nicht als Code-Konstante

**Wichtige Funktionen**:
- Keine: Die Modellaufl√∂sungslogik wird vom OpenCode-Kern verarbeitet, oh-my-opencode bietet Konfiguration und Priorit√§tsdefinition

</details>
